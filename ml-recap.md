---
title: "æœºå™¨å­¦ä¹ å¤ä¹ "
date: 2020-06-25T14:26:24+08:00
draft: false
---

As we will see later, the  ğ¿2  loss corresponds to the assumption that our data was corrupted by Gaussian noise, whereas the  ğ¿1  loss corresponds to an assumption of noise from a Laplace distribution.

pytorch é‡Œé¢çš„ broadcast åŠŸèƒ½ï¼Œå¦‚æœæ˜¯ 3x2 + 2x1 åˆ™å¦‚ä½•?

å‚è€ƒèµ„æ–™ï¼š[ç†è§£çŸ©é˜µ](https://blog.csdn.net/myan/article/details/647511)

a vector norm is a function  ğ‘“  that maps a vector to a scalar,



å‚è€ƒèµ„æ–™ [å‘é‡å¾®ç§¯åˆ†ç³»åˆ—](https://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/)

https://blog.csdn.net/u010976453/article/details/54381248
https://www.cnblogs.com/pinard/p/10791506.html

å‚è€ƒ[åˆ†å¸ƒä¼°è®¡](https://towardsdatascience.com/estimating-distributions-nonparametric-713ccf0647b)

## éšæœºå˜é‡
éšæœºå˜é‡å®é™…ä¸Šæ˜¯ä¸€ä¸ªå‡½æ•°ã€‚åœ¨è¡¨ç¤ºæ—¶ï¼Œä¸€èˆ¬ä½¿ç”¨ Xã€‚åˆå¦‚ X(x) åˆ™è¡¨ç¤ºå‡½æ•° X çš„ x å‚æ•°ã€‚æˆ–è€… X = 1 åˆ™è¡¨ç¤ºå‡½æ•°Xçš„å€¼æ˜¯1çš„æ˜¯æ—¶å€™ã€‚

## å¤šç»´åº¦
ç»´åº¦ä¸Šå‡æ—¶ï¼Œè®¡ç®—ç§¯åœ¨æ•°å€¼ä¸Šé€æ¸å‡å°‘ç›´åˆ°0ã€‚ä¸åŒç»´åº¦çš„ç§¯æ²¡æœ‰æ¯”è¾ƒçš„æ„ä¹‰ã€‚
